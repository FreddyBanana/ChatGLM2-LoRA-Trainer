## ChatGLM2-LoRA-Trainer

### ç®€ä»‹ / Introduction

æœ¬ä»“åº“åˆ©ç”¨[peft]([huggingface/peft: ğŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning. (github.com)](https://github.com/huggingface/peft))åº“ä¸transformers.Trainerï¼Œå®ç°å¯¹ChatGLM/ChatGLM2çš„ç®€å•8bit LoRAå¾®è°ƒã€‚ï¼ˆå…¶å®ƒLLMåº”è¯¥ä¹Ÿè¡Œï¼Œåªè¦ç¨ä½œä¿®æ”¹ï¼‰

This repo uses [peft](https://github.com/huggingface/peft) and transformers.Trainer to achieve simple 8-bit LoRA fine-tuning for ChatGLM/ChatGLM2. ï¼ˆYou can also use this repo for other LLM with minor modificationsï¼‰



### å®‰è£…ä¾èµ– / Installing the dependencies

```
$ pip install -r requirement.txt
```

requirement.txtï¼š

```
datasets==2.13.1
protobuf
transformers==4.30.2
cpm_kernels
torch>=2.0
mdtex2html
sentencepiece
accelerate
git+https://github.com/huggingface/peft.git
bitsandbytes
loralib
scipy
```



### å‚æ•°/config

æ–‡ä»¶config.pyå‚æ•°å¦‚ä¸‹ï¼š

- **MICRO_BATCH_SIZE**ï¼Œæ¯å—GPUçš„batch sizeå¤§å°ã€‚
- **BATCH_SIZE**ï¼ŒçœŸæ­£çš„batch sizeï¼Œå½“æ¯ä¸ªbatchçš„å¤„ç†æ ·æœ¬æ•°è¾¾åˆ°BATCH_SIZEæ—¶ï¼Œè¿›è¡Œæ¢¯åº¦æ›´æ–°ã€‚
- **EPOCHS**ï¼Œæ€»è®­ç»ƒä»£æ•°ã€‚
- **WARMUP_STEPS**ï¼Œé¢„çƒ­æ­¥æ•°ã€‚
- **LEARNING_RATE**ï¼Œå­¦ä¹ ç‡ã€‚
- **CUTOFF_LEN**ï¼Œtokenizeræˆªæ–­é•¿åº¦ã€‚
- **LORA_R**ï¼ŒLoRAä½ç§©çš„ç§©æ•°ã€‚
- **LORA_ALPHA**ï¼ŒLoRAçš„alphaã€‚
- **LORA_DROPOUT**ï¼ŒLoRAå±‚çš„Dropoutç‡ã€‚
- **MODEL_NAME**ï¼Œæ¨¡å‹åç§°ï¼ˆhuggingfaceä»“åº“åœ°å€ï¼‰ã€‚
- **LOGGING_STEPS**ï¼Œæ—¥å¿—æ­¥æ•°ï¼Œå³è®­ç»ƒçš„æ—¶å€™è¾“å‡ºlossçš„é—´éš”æ­¥æ•°ã€‚
- **OUTPUT_DIR**ï¼Œè¾“å‡ºLoRAæƒé‡çš„å­˜æ”¾æ–‡ä»¶å¤¹ä½ç½®ã€‚
- **DATA_PATH**ï¼Œæ•°æ®é›†æ–‡ä»¶ä½ç½®ã€‚
- **DATA_TYPE**ï¼Œæ•°æ®é›†æ–‡ä»¶ç±»å‹ï¼Œå¯é€‰jsonæˆ–txtã€‚
- **SAVE_STEPS**ï¼Œä¿å­˜LoRAæƒé‡çš„é—´éš”æ­¥æ•°ã€‚
- **SAVE_TOTAL_LIMIT**ï¼Œä¿å­˜LoRAæƒé‡æ–‡ä»¶çš„æ€»æ•°ï¼ˆä¸åŒ…æ‹¬æœ€ç»ˆæƒé‡ï¼‰ã€‚
- **PROMPT**ï¼Œæ¨ç†æ—¶çš„promptã€‚
- **TEMPERATURE**ï¼Œæ¨ç†æ—¶çš„æ¸©åº¦ï¼Œè°ƒæ•´æ¨¡å‹çš„åˆ›é€ åŠ›ã€‚
- **LORA_CHECKPOINT_DIR**ï¼Œå¾…æ¨ç†LoRAæƒé‡çš„æ–‡ä»¶å¤¹ä½ç½®ã€‚



The parameters in config.py are as follows:

- **MICRO_BATCH_SIZE**ï¼Œbatch size on each deviceã€‚
- **BATCH_SIZE**ï¼Œwhen the number of processed samples in each split batch reaches BATCH_SIZE, update the gradient.
- **EPOCHS**ï¼Œtraining epochsã€‚
- **WARMUP_STEPS**ï¼Œwarmup stepsã€‚
- **LEARNING_RATE**ï¼Œlearning rate of fine-tuningã€‚
- **CUTOFF_LEN**ï¼Œtruncation length of tokenizerã€‚
- **LORA_R**ï¼ŒLora low rankã€‚
- **LORA_ALPHA**ï¼ŒLora Alphaã€‚
- **LORA_DROPOUT**ï¼ŒLora dropoutã€‚
- **MODEL_NAME**ï¼Œmodel name (huggingface repo address)ã€‚
- **LOGGING_STEPS**ï¼Œthe number of interval steps for outputting loss during trainingã€‚
- **OUTPUT_DIR**ï¼Œthe storage folder location for LoRA weightsã€‚
- **DATA_PATH**ï¼Œthe location of your dataset fileã€‚
- **DATA_TYPE**ï¼Œthe type of your dataset file, including json and txtã€‚
- **SAVE_STEPS**ï¼Œthe number of interval steps to save LoRA weightsã€‚
- **SAVE_TOTAL_LIMIT**ï¼Œthe total number of LoRA weight files saved (excluding the final one)ã€‚
- **PROMPT**ï¼Œyour prompt when inferenceã€‚
- **TEMPERATURE**ï¼Œthe temperature when inference, adjusting the creativity of LLMã€‚
- **LORA_CHECKPOINT_DIR**ï¼Œfolder location for LoRA weights to be inferredã€‚



### æ•°æ®é›†æ–‡ä»¶/Dataset files

### 1ï¼‰json

jsonæ–‡ä»¶æ ¼å¼å¦‚ä¸‹ï¼š

The JSON file format is as followsï¼š

```json
{"context":question1, "target":answer1}{"context":question2, "target":answer2}...
```

ç”Ÿæˆpromptå¦‚ä¸‹ï¼ˆå¯ä¿®æ”¹ï¼‰ï¼š

Generate prompt as follows (modifiable)ï¼š

```
"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction: 
{data_point["context"]}

### Answer: 
{data_point["target"]}"""
```



### 2ï¼‰txt

txtæ–‡ä»¶æ ¼å¼å¦‚ä¸‹ï¼š

The txt file format is as followsï¼š

```
sentence1
sentence2
sentence3
...
```

ç”Ÿæˆpromptå¦‚ä¸‹ï¼ˆå¯ä¿®æ”¹ï¼‰ï¼š

Generate prompt as follows (modifiable)ï¼š

```
"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### 
{data_point["text"]}"""
```



### ä½¿ç”¨æ–¹æ³• / Usage

#### 1ï¼‰è®­ç»ƒ/train

```
$ sh train.sh
```

train.shï¼š

```shell
python main.py \
	--MICRO_BATCH_SIZE 8 \
	--BATCH_SIZE 16 \
	--EPOCHS 50 \
	--LEARNING_RATE 2e-5 \
	--CUTOFF_LEN 256 \
	--LORA_R 16 \
	--MODEL_NAME THUDM/chatglm2-6b \
	--OUTPUT_DIR ./output_model \
	--DATA_PATH ./new_train.json \
	--DATA_TYPE json \
	--SAVE_STEPS 1000

```



#### 2ï¼‰æ¨ç†/inference

```
$ sh inference.sh
```

inference.shï¼š

```shell
python inference.py \
	--CUTOFF_LEN 256 \
	--MODEL_NAME THUDM/chatglm2-6b \
	--LORA_CHECKPOINT_DIR ./output_model/checkpoint-4000/ \
	--PROMPT put your prompt here
```



### å‚è€ƒ / Reference

[Fine_Tuning_LLama | Kaggle](https://www.kaggle.com/code/gunman02/fine-tuning-llama?scriptVersionId=128204744)

[mymusise/ChatGLM-Tuning: ä¸€ç§å¹³ä»·çš„chatgptå®ç°æ–¹æ¡ˆ, åŸºäºChatGLM-6B + LoRA (github.com)](https://github.com/mymusise/ChatGLM-Tuning/tree/master)